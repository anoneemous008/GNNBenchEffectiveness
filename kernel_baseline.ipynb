{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load local dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.6.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'my_utils' from '/li_zhengdao/github/GenerativeGNN/my_utils.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import random\n",
    "import argparse\n",
    "import configparser\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch_sparse\n",
    "from torch import Tensor\n",
    "from torch.nn import Linear\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, to_networkx\n",
    "from typing import Union, Tuple\n",
    "from torch_geometric.typing import OptPairTensor, Adj, OptTensor, Size\n",
    "from torch_sparse import SparseTensor, matmul\n",
    "from torch_geometric.nn.conv import MessagePassing\n",
    "\n",
    "from ogb.linkproppred import PygLinkPropPredDataset, Evaluator\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import math\n",
    "\n",
    "\n",
    "from dataset_utils import node_feature_utils\n",
    "from dataset_utils.node_feature_utils import *\n",
    "import my_utils as utils\n",
    "\n",
    "importlib.reload(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['REDDIT-BINARY', 'REDDIT-MULTI-5K', 'COLLAB', 'IMDB-BINARY', 'IMDB-MULTI', 'NCI1', 'AIDS', 'ENZYMES', 'PROTEINS', 'DD', 'MUTAG', 'CSL', 'CIFAR10', 'MNIST', 'PPI', 'hiv', 'bace', 'bbpb', 'ogbg_molhiv', 'ogbg_ppa', 'PTC', 'QM9', 'ogbg_moltox21', 'ogbg-molbbbp', 'ogbg-molbace', 'syn_cc', 'syn_degree'])\n"
     ]
    }
   ],
   "source": [
    "# Load specific dataset:\n",
    "\n",
    "import sys,os\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "\n",
    "from PrepareDatasets import DATASETS\n",
    "import my_utils\n",
    "import dataset_utils\n",
    "\n",
    "\n",
    "print(DATASETS.keys())\n",
    "\"\"\"\n",
    "    'REDDIT-BINARY': RedditBinary,\n",
    "    'REDDIT-MULTI-5K': Reddit5K,\n",
    "    'COLLAB': Collab,\n",
    "    'IMDB-BINARY': IMDBBinary,\n",
    "    'IMDB-MULTI': IMDBMulti,\n",
    "    'ENZYMES': Enzymes,\n",
    "    'PROTEINS': Proteins,\n",
    "    'NCI1': NCI1,\n",
    "    'DD': DD,\n",
    "    \"MUTAG\": Mutag,\n",
    "    'CSL': CSL\n",
    "\"\"\"\n",
    "\n",
    "data_names = ['PROTEINS']\n",
    "data_names = ['DD']\n",
    "data_names = ['ENZYMES']\n",
    "data_names = ['NCI1']\n",
    "data_names = ['IMDB-MULTI']\n",
    "data_names = ['REDDIT-BINARY']\n",
    "data_names = ['CIFAR10']\n",
    "data_names = ['ogbg_molhiv']\n",
    "\n",
    "\n",
    "# NOTE:new kernel:\n",
    "data_names = ['MUTAG']\n",
    "data_names = ['DD', 'PROTEINS', 'ENZYMES']\n",
    "\n",
    "data_names = ['ogbg_moltox21','ogbg-molbace']\n",
    "data_names = []\n",
    "\n",
    "datasets_obj = {}\n",
    "for k, v in DATASETS.items():\n",
    "    if k not in data_names:\n",
    "        continue\n",
    "    \n",
    "    print('loaded dataset, name:', k)\n",
    "    dat = v(use_node_attrs=True)\n",
    "    datasets_obj[k] = dat\n",
    "    # print(type(dat.dataset.get_data()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed_dir:  DATA/syn_cc/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_cc_0.1_class5.pkl\n",
      "!!!! _dim_target:  5\n",
      "dataset len:  4096\n",
      "load splits: DATA/syn_cc/processed/syn_cc_0.1_class5_splits.json\n",
      "split counts: 10\n",
      "processed_dir:  DATA/syn_cc/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_cc_0.2_class5.pkl\n",
      "!!!! _dim_target:  5\n",
      "dataset len:  4096\n",
      "load splits: DATA/syn_cc/processed/syn_cc_0.2_class5_splits.json\n",
      "split counts: 10\n",
      "processed_dir:  DATA/syn_cc/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_cc_0.3_class5.pkl\n",
      "!!!! _dim_target:  5\n",
      "dataset len:  4096\n",
      "load splits: DATA/syn_cc/processed/syn_cc_0.3_class5_splits.json\n",
      "split counts: 10\n",
      "processed_dir:  DATA/syn_cc/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_cc_0.4_class5.pkl\n",
      "!!!! _dim_target:  5\n",
      "dataset len:  4096\n",
      "load splits: DATA/syn_cc/processed/syn_cc_0.4_class5_splits.json\n",
      "split counts: 10\n",
      "processed_dir:  DATA/syn_cc/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_cc_0.5_class5.pkl\n",
      "!!!! _dim_target:  5\n",
      "dataset len:  4096\n",
      "load splits: DATA/syn_cc/processed/syn_cc_0.5_class5_splits.json\n",
      "split counts: 10\n",
      "processed_dir:  DATA/syn_cc/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_cc_0.6_class5.pkl\n",
      "!!!! _dim_target:  5\n",
      "dataset len:  4096\n",
      "load splits: DATA/syn_cc/processed/syn_cc_0.6_class5_splits.json\n",
      "split counts: 10\n",
      "processed_dir:  DATA/syn_cc/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_cc_0.7_class5.pkl\n",
      "!!!! _dim_target:  5\n",
      "dataset len:  4096\n",
      "load splits: DATA/syn_cc/processed/syn_cc_0.7_class5_splits.json\n",
      "split counts: 10\n",
      "processed_dir:  DATA/syn_cc/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_cc_0.8_class5.pkl\n",
      "!!!! _dim_target:  5\n",
      "dataset len:  4096\n",
      "load splits: DATA/syn_cc/processed/syn_cc_0.8_class5_splits.json\n",
      "split counts: 10\n",
      "processed_dir:  DATA/syn_cc/processed\n",
      "load dataset !\n",
      "SynDataset load data_path: DATA/syn_cc_0.9_class5.pkl\n",
      "!!!! _dim_target:  5\n",
      "dataset len:  4096\n",
      "load splits: DATA/syn_cc/processed/syn_cc_0.9_class5_splits.json\n",
      "split counts: 10\n"
     ]
    }
   ],
   "source": [
    "# load syn cc datasets:\n",
    "\n",
    "\n",
    "def get_new_config():\n",
    "    return {'model': 'GIN', 'device': 'cuda:0', 'batch_size': 128, 'learning_rate': 0.001, 'classifier_epochs':\n",
    "    200, 'hidden_units': [64, 300, 300, 64], 'layer_num': 5, 'optimizer': 'Adam', \n",
    "    'scheduler': {'class': 'StepLR', 'args': {'step_size': 50, 'gamma': 0.5}}, \n",
    "    'loss': 'MulticlassClassificationLoss', 'train_eps': False, 'l2': 0.0, 'aggregation': 'mean', 'gradient_clipping': None, \n",
    "    'dropout': 0.5, 'early_stopper': {'class': 'Patience', 'args': {'patience': 30, 'use_loss': False}},\n",
    "    'shuffle': True, 'resume': False,\n",
    "    'additional_features': 'degree', 'node_attribute': False,\n",
    "    'shuffle_feature': False, 'roc_auc': True, 'use_10_fold': True, \n",
    "    'mol_split': False, 'dataset': 'syn_degree', \n",
    "    'config_file': 'gnn_comparison/config_GIN_degree.yml', \n",
    "    'experiment': 'endtoend', \n",
    "    'result_folder': 'results/result_0530_GIN_degree_syn_degree_0.1_class2', \n",
    "    'dataset_name': 'syn_degree', 'dataset_para': '0.1_class2', 'outer_folds': 10, \n",
    "    'outer_processes': 2, 'inner_folds': 5, 'inner_processes': 1, 'debug': True, 'ogb_evl': False, \n",
    "    'model_name': 'GIN', 'device': 'cuda:0', 'batch_size': 128,\n",
    "    'learning_rate': 0.001, 'classifier_epochs': 200,\n",
    "    'hidden_units': [64, 300, 300, 64], 'layer_num': 5,\n",
    "    'train_eps': False, 'l2': 0.0,\n",
    "    'aggregation': 'mean',\n",
    "    'gradient_clipping': None, 'dropout': 0.5, \n",
    "    'shuffle': True, 'resume': False, 'additional_features': 'degree',\n",
    "    'node_attribute': False, \n",
    "    'shuffle_feature': False, 'roc_auc': True, 'use_10_fold': True, \n",
    "    'mol_split': False, 'dataset_name': 'syn_degree', \n",
    "    'experiment': 'endtoend', 'result_folder': 'results/result_0530_GIN_degree_syn_degree_0.1_class2',\n",
    "    'dataset_para': '0.1_class2', 'outer_folds': 10, \n",
    "    'outer_processes': 2, 'inner_folds': 5, 'inner_processes': 1, 'debug': True, 'ogb_evl': False}\n",
    "\n",
    "\n",
    "cc_datasets = []\n",
    "for i in range(1, 10):\n",
    "    configs = get_new_config()\n",
    "    corrs = round(i/10.0, 1)\n",
    "    configs['dataset_para'] = f'{corrs}_class5'\n",
    "    cc_datasets.append(DATASETS['syn_cc'](config=configs))\n",
    "\n",
    "\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n",
      "[0.29738563]\n"
     ]
    }
   ],
   "source": [
    "# use mlp to train:\n",
    "import pickle as pk\n",
    "ccpath = '/li_zhengdao/github/GenerativeGNN/DATA/syn_cc/processed/graphwise_syn_cc_0.9_class5_add_avg_cc.pkl'\n",
    "\n",
    "with open(ccpath, 'rb') as f:\n",
    "    cc = pk.load(f)\n",
    "    print(len(cc))\n",
    "    print(cc[4000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({2: 1477, 1: 1182, 3: 1125, 0: 164, 4: 148})\n",
      "0.923828125\n"
     ]
    }
   ],
   "source": [
    "# check the balance of the dataset:\n",
    "\n",
    "d = cc_datasets[-1]\n",
    "\n",
    "labels = []\n",
    "for i in d.dataset:\n",
    "    labels.append(i.y.item())\n",
    "    \n",
    "from collections import Counter\n",
    "\n",
    "print(Counter(labels))\n",
    "\n",
    "print((1477+1182+1125)/4096.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_each_folder(dataset, fold_id, batch_size=1):\n",
    "    \n",
    "    fold_test = dataset.get_test_fold(fold_id, batch_size=batch_size, shuffle=True).dataset\n",
    "    fold_train, fold_val = dataset.get_model_selection_fold(fold_id, inner_idx=None,\n",
    "                                                                          batch_size=batch_size, shuffle=True)\n",
    "    fold_train = fold_train.dataset\n",
    "    fold_val = fold_val.dataset\n",
    "    \n",
    "    # train_G = [pyg_utils.to_networkx(d, node_attrs=['x']) for d in fold_train.get_subset()]\n",
    "    # test_G = [pyg_utils.to_networkx(d, node_attrs=['x']) for d in fold_test.get_subset()]\n",
    "    # print('x: ',train_G[0].nodes[0]['x'])\n",
    "    \n",
    "    train_adjs, test_adjs = [], []\n",
    "    train_y, test_y = [], []\n",
    "    \n",
    "    def node_fea_to_dict(node_fea):\n",
    "        res = {}\n",
    "        for i in range(node_fea.shape[0]):\n",
    "            res[i] = node_fea[i]\n",
    "        return res\n",
    "    print('train len:', len(fold_train))\n",
    "    print('test len:', len(fold_test))\n",
    "    print('total len: ', len(dataset.dataset))\n",
    "    \n",
    "    \n",
    "    if hasattr(fold_train, \"get_subset\"):\n",
    "        for d in fold_train.get_subset():\n",
    "\n",
    "            train_y.append(d.y.item())\n",
    "            train_adjs.append([d.to_numpy_array()])\n",
    "\n",
    "        for d in fold_test.get_subset():\n",
    "            test_y.append(d.y.item())\n",
    "            test_adjs.append([d.to_numpy_array()])\n",
    "            \n",
    "    else:\n",
    "        train_adjs = dataset.get_dense_adjs(fold_train)\n",
    "        test_adjs = dataset.get_dense_adjs(fold_test)\n",
    "        \n",
    "        for d in fold_train:\n",
    "            train_y.append(d.y)\n",
    "        # is_labeled = data.y == data.y\n",
    "        for d in fold_test:\n",
    "            test_y.append(d.y)\n",
    "            \n",
    "        train_y = torch.cat(train_y, dim=0)\n",
    "        test_y = torch.cat(test_y, dim=0)\n",
    "        \n",
    "        print('train y shape:', train_y.shape)\n",
    "        print('test y shape:', test_y.shape)\n",
    "    return train_adjs, test_adjs, train_y, test_y\n",
    "    # do not use val for kernel methods.\n",
    "#     for d in fold.dataset.get_subset():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform from networkx\n",
    "from grakel.utils import graph_from_networkx"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from grakel.datasets import fetch_dataset\n",
    "from grakel.kernels import ShortestPath\n",
    "import numpy as np\n",
    "from grakel.kernels import WeisfeilerLehman,SubgraphMatching\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from grakel import Graph\n",
    "from grakel import utils as g_utils\n",
    "\n",
    "import networkx as nx\n",
    "# Loads the MUTAG dataset\n",
    "\n",
    "\n",
    "\n",
    "# Define the Weisfeiler-Lehman kernel\n",
    "\n",
    "def train_with_wl_kernel(wl_kernel, train_adj_matrices, test_adj_matrices, train_labels, test_labels):\n",
    "    y_train = train_labels\n",
    "    y_test = test_labels\n",
    "    \n",
    "    \n",
    "    def transform_to_gr_graphs(adjs):\n",
    "        nx_gs = []\n",
    "        all_node_labels = []\n",
    "        for m in adjs:\n",
    "            if isinstance(m, list) or len(m.shape) > 2:\n",
    "                nx_g = nx.from_numpy_array(m[0])\n",
    "                N = m[0].shape[0]\n",
    "            else:\n",
    "                nx_g = nx.from_numpy_array(m)\n",
    "                N = m.shape[0]\n",
    "                \n",
    "            node_labels = {i:0 for i in range(N)}\n",
    "            nx_gs.append(nx_g)\n",
    "            all_node_labels.append(node_labels)\n",
    "        \n",
    "        gr_graphs =  [g for g in g_utils.graph_from_networkx(nx_gs, as_Graph=True)]\n",
    "        \n",
    "        for i, g in enumerate(gr_graphs):\n",
    "            g.node_labels = all_node_labels[i]\n",
    "            \n",
    "        return gr_graphs\n",
    "    \n",
    "    \n",
    "    train_graphs = transform_to_gr_graphs(train_adj_matrices)\n",
    "    test_graphs = transform_to_gr_graphs(test_adj_matrices)\n",
    "    \n",
    "    wl_kernel.fit(train_graphs)\n",
    "\n",
    "    # Transform the graphs using the Weisfeiler-Lehman kernel\n",
    "    X_train = wl_kernel.transform([graph for graph in train_graphs])\n",
    "    X_test = wl_kernel.transform([graph for graph in test_graphs])\n",
    "\n",
    "    # Train an SVM classifier on the transformed training data\n",
    "    svm = SVC()\n",
    "    \n",
    "    if y_train.dim() > 1 and y_train.shape[-1] > 1:\n",
    "        multilabel_classifier = MultiOutputClassifier(svm, n_jobs=-1)\n",
    "\n",
    "        # Fit the data to the Multilabel classifier\n",
    "        not_label = y_train == y_train\n",
    "        \n",
    "        multilabel_classifier = multilabel_classifier.fit(X_train[not_label], y_train[not_label])\n",
    "\n",
    "        # Get predictions for test data\n",
    "        y_test_pred = multilabel_classifier.predict(X_test)\n",
    "    else:\n",
    "        svm.fit(X_train, y_train)\n",
    "        # Predict labels on the validation and test data using the trained SVM classifier\n",
    "        y_test_pred = svm.predict(X_test)\n",
    "\n",
    "    # Calculate the accuracy of the SVM classifier on the validation and test data\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    \n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "\n",
    "# MUTAG = fetch_dataset(\"MUTAG\", verbose=False)\n",
    "# G, y = MUTAG.data, MUTAG.target\n",
    "# print('G10:', G[0])\n",
    "\n",
    "def train_with_kernel(gk, dataset_name):\n",
    "    res=[]\n",
    "    for i in range(10):\n",
    "        G_train, G_test, y_train, y_test = get_each_folder(dataset_name, i)\n",
    "        \n",
    "        # G_train = [g for g in graph_from_networkx(G_train,node_labels_tag='x')]\n",
    "        # G_test = [g for g in graph_from_networkx(G_test,node_labels_tag='x')]\n",
    "        # print('G_train 10:',G_train[:10])\n",
    "        \n",
    "        # G_train, G_test, y_train, y_test = train_test_split(G_train, y_train, test_size=0.1)\n",
    "        # Uses the shortest path kernel to generate the kernel matrices\n",
    "        if isinstance(gk, WeisfeilerLehman) or isinstance(gk, SubgraphMatching):\n",
    "            res.append(train_with_wl_kernel(gk,  G_train, G_test, y_train, y_test))\n",
    "        else:\n",
    "            K_train = gk.fit_transform(G_train)\n",
    "            K_test = gk.transform(G_test)\n",
    "\n",
    "            # Uses the SVM classifier to perform classification\n",
    "            clf = SVC(kernel=\"precomputed\")\n",
    "            clf.fit(K_train, y_train)\n",
    "            y_pred = clf.predict(K_test)\n",
    "\n",
    "            # Computes and prints the classification accuracy\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            res.append(acc)\n",
    "            # print(\"Accuracy:\", str(round(acc*100, 2)) + \"%\")\n",
    "        \n",
    "    res = np.array(res)\n",
    "    print(f'Acc, mean: {round(np.mean(res)*100, 4)}, std: {round(100*np.std(res),4)}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3317\n",
      "221\n",
      "train len: 3317\n",
      "test len: 410\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3317])\n",
      "test y shape: torch.Size([410])\n",
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3317\n",
      "266\n",
      "train len: 3317\n",
      "test len: 410\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3317])\n",
      "test y shape: torch.Size([410])\n",
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3317\n",
      "3804\n",
      "train len: 3317\n",
      "test len: 410\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3317])\n",
      "test y shape: torch.Size([410])\n",
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3317\n",
      "1822\n",
      "train len: 3317\n",
      "test len: 410\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3317])\n",
      "test y shape: torch.Size([410])\n",
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3317\n",
      "1455\n",
      "train len: 3317\n",
      "test len: 410\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3317])\n",
      "test y shape: torch.Size([410])\n",
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3317\n",
      "2136\n",
      "train len: 3317\n",
      "test len: 410\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3317])\n",
      "test y shape: torch.Size([410])\n",
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3318\n",
      "3158\n",
      "train len: 3318\n",
      "test len: 409\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3318])\n",
      "test y shape: torch.Size([409])\n",
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3318\n",
      "1844\n",
      "train len: 3318\n",
      "test len: 409\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3318])\n",
      "test y shape: torch.Size([409])\n",
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3318\n",
      "3211\n",
      "train len: 3318\n",
      "test len: 409\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3318])\n",
      "test y shape: torch.Size([409])\n",
      "<class 'dict'>\n",
      "2\n",
      "<class 'list'>\n",
      "3318\n",
      "331\n",
      "train len: 3318\n",
      "test len: 409\n",
      "total len:  4096\n",
      "train y shape: torch.Size([3318])\n",
      "test y shape: torch.Size([409])\n",
      "Acc, mean: 13.0128, std: 1.5516\n"
     ]
    }
   ],
   "source": [
    "# MUTAG\n",
    "\n",
    "from grakel.kernels import ShortestPath, WeisfeilerLehman, SubgraphMatching\n",
    "\n",
    "for d in cc_datasets:\n",
    "    train_with_kernel(WeisfeilerLehman(n_iter=25), d)\n",
    "    break\n",
    "    # train_with_kernel(SubgraphMatching(), data_names[0])\n",
    "    # train_with_kernel(ShortestPath(normalize=True, with_labels=False), data_names[0])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: MUTAG\n",
      "Acc, mean: 85.117, std: 8.0719\n",
      "Acc, mean: 85.117, std: 8.0719\n",
      "Acc, mean: 78.7719, std: 6.575\n"
     ]
    }
   ],
   "source": [
    "# MUTAG\n",
    "\n",
    "from grakel.kernels import ShortestPath, WeisfeilerLehman, SubgraphMatching\n",
    "\n",
    "\n",
    "\n",
    "print('kernel on:', data_names[0])\n",
    "\n",
    "gks = [ShortestPath(normalize=True, with_labels=False),\n",
    "      WeisfeilerLehman(n_iter=5),\n",
    "      SubgraphMatching(normalize=True)]\n",
    "\n",
    "train_with_kernel(WeisfeilerLehman(n_iter=5), data_names[0])\n",
    "train_with_kernel(SubgraphMatching(), data_names[0])\n",
    "train_with_kernel(ShortestPath(normalize=True, with_labels=False), data_names[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: DD\n"
     ]
    }
   ],
   "source": [
    "# run:\n",
    "\n",
    "from grakel.kernels import ShortestPath, WeisfeilerLehman, SubgraphMatching\n",
    "\n",
    "\n",
    "for name in data_names:\n",
    "    print('kernel on:', name)\n",
    "    gks = [ShortestPath(normalize=True, with_labels=False),\n",
    "        WeisfeilerLehman(n_iter=5),\n",
    "        SubgraphMatching(normalize=True)]\n",
    "\n",
    "    train_with_kernel(WeisfeilerLehman(n_iter=5), data_names[0])\n",
    "    train_with_kernel(SubgraphMatching(), data_names[0])\n",
    "    train_with_kernel(ShortestPath(normalize=True, with_labels=False), data_names[0])\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM for $|V|+\\alpha|E|$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_utils.node_feature_utils import graph_invariant\n",
    "\n",
    "def train_simple_svm(kernel_name, dataset_name, folds_num=10):\n",
    "    res = []\n",
    "    auc = []\n",
    "    for i in range(folds_num):\n",
    "        train_adjs, test_adjs, train_y, test_y= get_each_folder(dataset_name, i)\n",
    "        # NOTE: adj -> graph_features\n",
    "        \n",
    "        train_x = [graph_invariant(adj=adj[0]) for adj in train_adjs]\n",
    "        test_x = [graph_invariant(adj=adj[0]) for adj in test_adjs]\n",
    "        Classifier = SVC(kernel=kernel_name)\n",
    "        Classifier.fit(train_x, train_y)\n",
    "        y_pred = Classifier.predict(test_x)\n",
    "        # Computes and prints the classification accuracy\n",
    "        acc = accuracy_score(test_y, y_pred)\n",
    "        rocauc = roc_auc_score(test_y, y_pred)\n",
    "        res.append(acc)\n",
    "        auc.append(rocauc)\n",
    "        # print(\"Accuracy:\", str(round(acc*100, 2)) + \"%\")\n",
    "         \n",
    "    res = np.array(res)\n",
    "    auc = np.array(auc)\n",
    "    \n",
    "    print(f'Acc, mean: {round(np.mean(res)*100, 4)}, std: {round(100*np.std(res),4)}')\n",
    "    print(f'ROCAUC, mean: {round(np.mean(auc)*100, 4)}, std: {round(100*np.std(auc),4)}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc, mean: 85.117, std: 8.0719\n",
      "Acc, mean: 82.9532, std: 7.7978\n",
      "Acc, mean: 86.2281, std: 8.5031\n",
      "Acc, mean: 66.4912, std: 2.2807\n"
     ]
    }
   ],
   "source": [
    "# MUTAG:\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    train_simple_svm(kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: PROTEINS\n",
      "Acc, mean: 69.1739, std: 4.5649\n",
      "Acc, mean: 59.5681, std: 0.1659\n",
      "Acc, mean: 72.5, std: 2.5759\n",
      "Acc, mean: 59.1144, std: 5.4115\n"
     ]
    }
   ],
   "source": [
    "# Proteins:\n",
    "\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    print('kernel used:', kr)\n",
    "    train_simple_svm(kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: DD\n",
      "kernel used: linear\n",
      "Acc, mean: 75.5556, std: 2.3243\n",
      "kernel used: poly\n",
      "Acc, mean: 65.8743, std: 2.5075\n",
      "kernel used: rbf\n",
      "Acc, mean: 76.0648, std: 3.2092\n",
      "kernel used: sigmoid\n",
      "Acc, mean: 62.3801, std: 12.4925\n"
     ]
    }
   ],
   "source": [
    "# DD:\n",
    "\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    print('kernel used:', kr)\n",
    "    train_simple_svm(kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: ENZYMES\n",
      "Acc, mean: 22.3333, std: 4.6068\n",
      "Acc, mean: 20.5, std: 3.5785\n",
      "Acc, mean: 22.3333, std: 3.8152\n",
      "Acc, mean: 13.3333, std: 4.5947\n"
     ]
    }
   ],
   "source": [
    "# ENZYMES:\n",
    "\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    train_simple_svm(kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: NCI1\n",
      "Acc, mean: 62.5061, std: 1.8026\n",
      "Acc, mean: 59.927, std: 1.4106\n",
      "Acc, mean: 62.5061, std: 1.7927\n",
      "Acc, mean: 37.7859, std: 2.1462\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# NCI1:\n",
    "\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['linear', 'poly', 'rbf', 'sigmoid']:\n",
    "    train_simple_svm(kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: COLLAB\n",
      "Acc, mean: 53.88, std: 1.1737\n",
      "Acc, mean: 60.82, std: 0.6161\n",
      "Acc, mean: 61.66, std: 1.1351\n"
     ]
    }
   ],
   "source": [
    "# COLLAB:\n",
    "\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['linear', 'poly', 'rbf']:\n",
    "    train_simple_svm(kr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: IMDB-MULTI\n",
      "Acc, mean: 33.4, std: 0.8138\n",
      "Acc, mean: 40.3333, std: 3.7977\n"
     ]
    }
   ],
   "source": [
    "# IMDB-M:\n",
    "\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['poly', 'rbf']:\n",
    "    train_simple_svm(kr, data_names[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: CIFAR10\n",
      "idxs keys: dict_keys(['train', 'validation'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:The OGB package is out of date. Your version is 1.3.2, while the latest version is 1.3.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc, mean: 12.31, std: 0.0\n",
      "idxs keys: dict_keys(['train', 'validation'])\n",
      "Acc, mean: 14.62, std: 0.0\n"
     ]
    }
   ],
   "source": [
    "# CIFAR10\n",
    "\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['poly', 'rbf']:\n",
    "    train_simple_svm(kr, data_names[0], folds_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: ogbg_molhiv\n",
      "idxs keys: dict_keys(['train', 'validation'])\n",
      "Acc, mean: 96.8393, std: 0.0\n",
      "ROCAUC, mean: 50.0, std: 0.0\n",
      "idxs keys: dict_keys(['train', 'validation'])\n",
      "Acc, mean: 96.8393, std: 0.0\n",
      "ROCAUC, mean: 50.0, std: 0.0\n"
     ]
    }
   ],
   "source": [
    "# HIV\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['poly', 'rbf']:\n",
    "    train_simple_svm(kr, data_names[0], folds_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: REDDIT-BINARY\n",
      "idxs keys: dict_keys(['train', 'validation'])\n",
      "Acc, mean: 79.0, std: 0.0\n",
      "ROCAUC, mean: 79.0, std: 0.0\n"
     ]
    }
   ],
   "source": [
    "# REDDIT-B\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['rbf']:\n",
    "    train_simple_svm(kr, data_names[0], folds_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel on: REDDIT-BINARY\n",
      "idxs keys: dict_keys(['train', 'validation'])\n",
      "Acc, mean: 61.0, std: 0.0\n",
      "ROCAUC, mean: 61.0, std: 0.0\n"
     ]
    }
   ],
   "source": [
    "# REDDIT-B\n",
    "print('kernel on:', data_names[0])\n",
    "for kr in ['poly']:\n",
    "    train_simple_svm(kr, data_names[0], folds_num=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
